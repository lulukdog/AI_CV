00:19:10	Brian:	j
00:19:14	Brian:	矩阵？
00:19:14	徐楠:	1
00:19:14	卢宇宙:	1
00:19:16	邓 奇:	可以
00:19:16	旭生 孙:	1
00:19:17	鄢新义:	1
00:19:18	潘力:	1
00:19:20	刘志强:	1
00:19:20	陈韵莹:	可以
00:19:20	王睿:	1
00:19:20	薛毓铨:	1
00:19:53	刘志强:	1
00:19:54	陈韵莹:	1
00:19:55	邓 奇:	可以
00:19:55	潘力:	1
00:20:43	唐陈:	矩阵
00:21:00	刘志强:	1\
00:21:00	卢宇宙:	1
00:21:01	斯太康:	1
00:21:01	陈韵莹:	1
00:21:01	刘志强:	1
00:21:02	徐楠:	1
00:21:02	邓 奇:	可以
00:21:03	薛毓铨:	可以
00:21:14	wuxi:	X0 = 1?
00:22:06	高格格:	1
00:22:07	刘志强:	1
00:22:08	陈悦:	1
00:22:08	卢宇宙:	1
00:22:08	严广宇:	1
00:22:08	徐楠:	1
00:22:08	陈韵莹:	1
00:23:25	何川:	1
00:23:27	薛毓铨:	没有
00:23:28	斯太康:	1
00:23:36	严广宇:	0
00:24:55	陈韵莹:	1
00:24:55	卢宇宙:	1
00:24:56	潘力:	1
00:24:57	鄢新义:	1
00:24:57	徐楠:	1
00:24:58	何川:	1
00:24:58	邓 奇:	1
00:24:59	斯太康:	1
00:25:00	薛毓铨:	ok
00:25:03	Brian:	1
00:25:04	刘志强:	1
00:26:06	陈韵莹:	1
00:26:07	斯太康:	1
00:26:08	卢宇宙:	1
00:26:08	邱松涛:	1
00:26:10	陈悦:	1
00:26:12	Xihui Ju:	1
00:27:53	潘力:	1
00:27:57	高格格:	1
00:28:18	李涛:	为什么距离要尽可能的大
00:28:19	齐鹏飞:	这是建立在一条线分不准的情况吗
00:28:20	Baoce SUN:	边界
00:28:33	潘力:	loss func最小
00:28:35	潘力:	？
00:28:40	Yimin MA:	如果有个圆点在左上角一堆叉的里面呢？
00:28:40	鄢新义:	不是吧
00:28:41	李涛:	不会有过拟合的情况吗
00:28:42	邓 奇:	这两个平面要平行吗？
00:28:50	刘权:	只要不平行就会相交吧。。怎么尽可能的大
00:29:18	夏敏:	绿的
00:29:19	陈韵莹:	绿
00:29:22	肖瑶萍:	绿
00:29:23	鄢新义:	应该要平行吧
00:29:24	潘力:	绿的
00:29:36	包颖:	要让两部分数据尽可能得分离开
00:29:57	李涛:	1
00:29:58	陈韵莹:	1
00:29:58	高格格:	1
00:29:59	彭冰:	1
00:30:00	邓 奇:	1
00:30:00	卢宇宙:	1
00:30:00	刘权:	好了
00:30:01	鄢新义:	1
00:30:02	刘志强:	1
00:30:02	唐陈:	1
00:30:03	包颖:	1
00:30:20	张帅:	如果数据分类的边界不是直线呢？
00:30:32	唐陈:	支持向量
00:30:52	鄢新义:	还是不理解
00:31:03	高格格:	不是直线的还没讲到呢
00:31:13	王奕楷:	不是直线就换个kernel，专心听老师讲吧
00:31:33	鄢新义:	懂了
00:31:57	唐陈:	大佬们，你们那声音有卡顿吗？
00:32:01	卢宇宙:	没有
00:32:03	邓 奇:	寻找支持向量就我们要干的？
00:32:03	陈韵莹:	没有
00:32:04	潘力:	不卡
00:32:06	邓 奇:	不卡
00:32:07	马玄:	没有
00:32:16	夏敏:	不卡
00:32:19	唐陈:	好的，那可能是我这边
00:33:39	薛毓铨:	hao 
00:33:40	Baoce SUN:	为什么正负
00:33:40	郑兴纯:	1
00:33:41	夏敏:	1
00:33:45	刘志强:	1
00:33:47	鄢新义:	y=wx+b
00:33:47	夏楠:	1
00:33:50	夏敏:	距离
00:33:57	Baoce SUN:	明白了
00:34:03	李涛:	中间没有样本吗
00:34:55	鄢新义:	1
00:34:56	陈韵莹:	1
00:34:57	刘志强:	1
00:34:58	邓 奇:	1
00:34:58	李涛:	1
00:34:58	潘力:	1
00:35:52	邓凯:	ok
00:35:52	斯太康:	1
00:35:54	Baoce SUN:	没问题
00:36:19	Administrator:	为什么y=1 的时候 是 wi*x+b?
00:36:24	邓凯:	最大间隔？
00:36:32	Administrator:	定义的吗
00:36:49	卢宇宙:	2平面距离
00:36:52	陈智杰:	距离
00:37:43	何川:	0
00:37:55	qinzongding:	公式是怎么来的？
00:38:02	Administrator:	为什么y=1 的时候 是 wi*x+b?
00:38:19	qinzongding:	r的哪个
00:38:19	齐鹏飞:	soga
00:38:22	Baoce SUN:	分子的最大值为2？
00:38:26	yangfanqihang:	2?
00:38:38	李涛:	点到直线的距离，假设点在哪里
00:39:28	邓凯:	分子呢
00:39:29	斯太康:	1
00:39:30	Baoce SUN:	2是最小值？
00:39:31	潘力:	1
00:39:31	卢宇宙:	1
00:39:55	杨洋:	就是点到平面的距离
00:40:12	Baoce SUN:	1
00:40:15	陈悦:	1
00:40:15	卢宇宙:	1
00:40:15	齐鹏飞:	1
00:41:14	陈韵莹:	1
00:41:16	卢宇宙:	1
00:41:16	旭生 孙:	1
00:41:23	Administrator:	||w|| 一般大于2还是小于2
00:41:55	卢宇宙:	最小值？
00:42:24	卢宇宙:	1
00:42:35	陈韵莹:	1
00:42:37	陈悦:	1
00:42:37	齐鹏飞:	1
00:42:37	旭生 孙:	1
00:42:44	斯太康:	1
00:43:17	陈智杰:	最大转最小那边再说一下吧
00:43:22	陈智杰:	没理解过来
00:43:30	卢宇宙:	倒数
00:43:41	齐鹏飞:	svm只是二分类？
00:43:54	齐鹏飞:	ok
00:46:20	彭冰:	1
00:46:20	Administrator:	这个A是乘子？
00:46:22	高格格:	1
00:46:22	唐明利:	1
00:46:26	陈韵莹:	1
00:46:30	陈悦:	1
00:46:30	旭生 孙:	拉格朗日没太懂
00:46:32	唐陈:	1
00:46:33	旭生 孙:	咋变换的
00:46:40	彭冰:	第三周作业要好好搞啊
00:46:44	马玄:	平面方程的b是不是被忽略了？
00:46:46	陈悦:	构造合并目标函数的
00:47:04	邓凯:	条件极值
00:47:14	高格格:	KKT
00:47:14	唐陈:	限制条件下求极值
00:47:15	罗家伟:	lambda不是在1/2这边嘛
00:47:25	孟明:	兰姆达是随便选的还是算出来的
00:47:26	马玄:	但是求距离的公式里面没有b
00:47:28	杨洋:	拉格朗日乘子是什么
00:47:29	旭生 孙:	OK了
00:48:00	D:	手推svm不用求解是嘛
00:48:07	旭生 孙:	多元函数求极值 想起来了
00:48:09	邓凯:	今天讲核函数吗
00:48:19	陈韵莹:	ok
00:48:20	陈悦:	1
00:48:21	唐陈:	1
00:48:21	肖瑶萍:	1
00:48:21	卢宇宙:	1
00:48:22	李涛:	1
00:48:22	夏敏:	1
00:49:09	罗家伟:	曲面？
00:50:11	Brian:	这个算凸优化么？
00:51:31	旭生 孙:	为什么减掉一个参数就会有这种变化呢
00:51:51	罗家伟:	epsilon是什么来的？
00:51:57	陈韵莹:	是算做一个平移吗？
00:52:05	李涛:	不那么严格的目的是什么，防止过拟合吗
00:52:10	陈智杰:	那不是相当于margin变小了的SVM吗
00:52:21	华涓 陈:	这个margin参数是对x0，x1...都加的么
00:52:30	唐陈:	regu
00:52:31	王睿:	惩罚项
00:52:35	齐鹏飞:	有没有可能找不到可以分开两拨数据的平面
00:52:39	夏敏:	带有soft  margin是不是只用于回归，不用于分类？
00:53:27	陈智杰:	C后面的是啥啊？看不清楚。。
00:53:48	斯太康:	C是啥
00:53:57	陈悦:	惩罚系数
00:54:08	彭冰:	过敏是啥意思
00:54:28	彭冰:	1
00:54:30	Tang JM:	过拟合 不是 过敏
00:54:42	彭冰:	是 过拟合吗
00:54:48	彭冰:	哦
00:55:13	陈韵莹:	这里的m是什么？
00:55:21	旭生 孙:	惩罚越小越不容易过拟合吗？
00:55:22	匡鸿深:	 1
00:55:23	刘权:	为什么c越大惩罚越大。。
00:55:27	Administrator:	怎么体现对误分类的惩罚的
00:55:37	华涓 陈:	m是样本数？
00:55:38	陈悦:	因为是求最小值啊
00:56:21	杨洋:	惩罚越大，是越可能过拟合，还是越不可能过拟合？
00:56:23	Administrator:	怎么体现对误分类的惩罚的
00:56:25	李涛:	之前正则化的时候，减去的越多，拟合的效果不就越差吗，这里怎么就反过来了
00:57:05	wuxi:	这里min的有两项，c调节两部分的比重
00:57:33	何川:	下标i，是说对于每一个类都有一个容差量？
00:57:35	旭生 孙:	有点晕 板书是c越大越可能啊
00:57:41	qinzongding:	C 大匙过拟还是不过拟？老师口误了吗
00:57:42	Administrator:	老师，这个惩罚项怎么理解
00:58:00	qinzongding:	好的
00:58:05	wuxi:	C越大相当于后一项越关键
00:58:21	wuxi:	第一项的抑制过拟合的效果就越差
00:58:24	刘权:	这个kethy怎么确定的
00:59:08	蒋开文:	一直想知到老师翻的那本书叫什么？
00:59:20	齐鹏飞:	那是笔记
01:01:36	Song:	为什么不用微积分曲线去拟合呢？
01:03:09	Administrator:	0
01:03:12	卢宇宙:	0
01:04:05	Song:	哪个地方是核？
01:04:08	陈智杰:	似懂非懂。。。
01:04:11	Song:	是fi吗？
01:04:15	刘志强:	不太懂
01:04:19	齐鹏飞:	sigma重要吗还是常数
01:04:35	wuxi:	高斯核？
01:04:36	yangfanqihang:	分类，离散？
01:04:51	邓凯:	可以理解成把x映射到更高维 ，变成线性可分吗
01:04:55	杨洋:	老师请继续往下讲，完整讲完核函数
01:06:14	Administrator:	老师，为什么要用核函数，直接用求距离可以吗
01:07:34	Song:	线性是做差吗，中间打个点是什么意思？
01:07:46	李涛:	最开始讲的是第二种吗
01:08:14	陈智杰:	那就是激活函数？
01:08:21	杨洋:	核函数怎样与SVM结合是不是还没有讲？
01:08:26	匡鸿深:	这些核能分类所有的场景吗
01:08:43	严广宇:	没有万能的核……
01:09:02	邓凯:	本质应该就是映射为高维，在高维线性可分
01:09:03	旭生 孙:	应用场景不同的话决定了选用平面还是选用核？
01:09:39	Yimin MA:	所以这些都比CNN来得实践效果差？
01:09:40	Song:	有人论证过SVM级联跟CNN是等价的吗？
01:10:17	Brian:	核函数不用推到吧。。
01:10:43	唐陈:	1
01:10:49	Brian:	1
01:12:29	王奕楷:	感觉是不是现在svm的应用已经比较少了
01:12:35	杨洋:	老师能不讲一个例子，比如高斯核函数，怎样应用在SVM里，然后SVM是怎么样具体实现分类的？
01:13:23	Song:	有人论证过NN与SVM的关系吗？
01:13:23	侠航 陈:	老师，上了规模一般是指多大量级
01:13:51	杨洋:	毕竟花了这么时间讲SVM，最后没听懂太亏了
01:13:54	陈韵莹:	老师能讲下svm进行多分类吗？
01:14:33	齐鹏飞:	svm主要分类，cnn主要提特征吧
01:14:45	王奕楷:	cnn可以直接分类
01:14:46	Song:	推导最重要是因为面试要考，还是以后可能会用？
01:15:16	Song:	好的，谢谢老师。
01:15:52	齐鹏飞:	1
01:15:53	Song:	第几周开始讲CNN呀？
01:15:56	Song:	猴急了
01:16:02	Song:	好的，谢谢老师。
01:16:07	旭生 孙:	CNN需要有基础吗
01:16:10	齐鹏飞:	有助于理解的确
01:16:12	王奕楷:	拉格朗日是真的挺有用，其实是多约束条件下优化必备的工具
01:16:17	彭冰:	SVM还是属于监督学习吧
01:16:42	薛毓铨:	瑟瑟发抖
01:16:44	薛毓铨:	看的
01:16:54	qinzongding:	SVM 有习题练习吗
01:17:05	杨俊桔:	老师，soft margin 需要掌握推导吗？
01:17:11	唐陈:	游泳？
01:17:51	zack:	推导SVM,服从条件里，y(wx+b)>=1,是怎么来的？
01:19:03	Song:	y=1是什么意思？
01:19:06	WW BB:	9:10回来
01:19:15	王睿:	公式中的s.t.什么意思
01:19:17	Song:	哦哦。
01:19:24	zack:	理解了
01:19:25	蒋开文:	BP的代码会说吗
01:19:53	杨洋:	老师，我理解代价函数里的惩罚项越大，theta 参数就越小，模型泛化效果越好，越不容易过拟合。我这个理解有问题吗
01:21:47	彭冰:	我怎么理解的是C越大越容易过拟合呢
01:22:18	陈悦:	c越大越容易拟合
01:22:35	罗家伟:	是c越大容易过拟合
01:22:46	旭生 孙:	是c越大惩罚越大吗
01:22:46	高格格:	这里的ξ和上节课LR正则项里面的β意义不同
01:23:20	旭生 孙:	我记得老师说惩罚越大越容易过拟合 有点懵了
01:23:21	陈悦:	c就是惩罚系数   越大对离群点惩罚越大
01:23:30	陈悦:	越容易过拟合
01:23:41	张帅:	我记得向量的手写体上面都带一个箭头，毕业好几年都忘了还有手写体的符号，方便平时演草用，哪可以学习一下
01:23:41	杨俊桔:	试着推导了一下，如果改为（1-ξ），那边界距离变为2（1-ξ）/（θ）²，这样从max改成min的话，ξ应该出现在分母，但最终结果是在分子，是做了什么变换吗？
01:23:43	马玄:	min ||θ||/2 里面有b项吗？一开始推导的距离公式里面没有b
01:23:47	唐陈:	1
01:24:21	旭生 孙:	惩罚越大不应该是曲线越平缓 不容易过拟合吗
01:25:51	李涛:	1
01:25:56	Baoce SUN:	1
01:26:02	Administrator:	为什么
01:26:02	高格格:	1
01:26:09	李涛:	更小就是过拟合
01:26:27	Administrator:	为什么1/2c越小就约不容易过拟合
01:26:33	齐鹏飞:	系数越大，theta就影响力就越小
01:26:34	杨洋:	我记得系数越小，曲线约平滑，越不容易过拟合
01:26:36	高格格:	go
01:27:14	杨洋:	懂了
01:27:15	齐鹏飞:	1
01:28:43	Tang JM:	多看几遍
01:30:19	刘权:	k-means比较简单
01:31:34	齐鹏飞:	平均值
01:31:35	陈靖韦:	threshold是啥
01:31:37	王睿:	1
01:31:44	Song:	再说一遍。
01:31:49	陈靖韦:	阈值是啥
01:32:42	高格格:	收敛
01:33:43	Administrator:	在这里收敛是啥意思，
01:33:47	Song:	这个是不是跟RANSAC有点类似？
01:33:50	刘权:	1
01:33:53	杨洋:	收敛？
01:33:55	高格格:	就是位置不动了
01:34:11	王奕楷:	就是迭代之间的位置没有很大的变动
01:34:16	侠航 陈:	中心点不动了
01:34:32	邓 奇:	1
01:34:32	夏敏:	1
01:34:32	卢宇宙:	1
01:34:33	刘权:	1
01:34:34	华涓 陈:	·1
01:34:34	侠航 陈:	1
01:34:35	旭生 孙:	每次比较都是用全部的样本吗
01:34:35	陈靖韦:	1
01:34:37	陈智杰:	X这个点是收敛出来的是吗
01:34:39	齐鹏飞:	老师，有没有好的初始化方法
01:34:40	Administrator:	初始化中心点有什么方法
01:34:55	杨洋:	具体实现的时候怎么判断中心点是否移动了，算距离吗？
01:36:04	王奕楷:	一般会设定个很小的距离阈值，距离变化小于阈值就认为收敛了
01:36:11	卢宇宙:	1
01:36:14	薛毓铨:	可以
01:36:15	马玄:	1
01:36:16	邓 奇:	1
01:36:16	Administrator:	pycharm？
01:36:19	孙小婷:	1
01:36:19	Administrator:	可以
01:36:30	薛毓铨:	字号有点小
01:36:31	Administrator:	老师，字体放大点咯
01:36:44	卢宇宙:	点file
01:36:47	卢宇宙:	setting
01:37:12	卢宇宙:	再大一点
01:37:14	Administrator:	再大点
01:37:15	唐陈:	28
01:37:22	高格格:	1
01:37:23	Administrator:	1
01:37:23	陈纬铨:	可以
01:37:25	李涛:	1
01:37:25	马玄:	1
01:37:27	卢宇宙:	1
01:37:27	斯太康:	1
01:38:30	陈靖韦:	导入cv2是要量化图片吗
01:38:55	陈靖韦:	run（）
01:40:22	卢宇宙:	字典
01:40:51	Song:	这个可以可视化看看吗？
01:46:58	齐鹏飞:	算距离
01:47:10	严广宇:	给每个点分类
01:50:23	杨洋:	老师写完了能把代码解释一下吗
01:50:25	严广宇:	df.loc有定义吗？
01:52:52	wuxi:	from_from
01:52:57	吴彬:	14
02:00:23	马玄:	break的条件是什么意思？
02:00:26	严广宇:	1
02:00:29	高格格:	1
02:00:30	孙小婷:	1
02:00:57	Tang JM:	0
02:00:57	唐陈:	1
02:00:58	严广宇:	1
02:03:27	Song:	没看懂。
02:03:30	齐鹏飞:	d是距离吗
02:03:31	刘权:	就是先选一个点，之后第二个初始点谁离第一个点距离越远越容易选到
02:03:38	彭冰:	方差
02:03:57	唐陈:	概率
02:04:36	邓凯:	1.随机选点 2. 选择最大距离的 ？ 重复2？
02:06:02	王奕楷:	就是距离越远概率越大
02:06:19	王奕楷:	目的是平均分布选取的点
02:09:07	包颖:	1
02:09:08	唐陈:	1
02:09:08	徐楠:	1
02:09:09	夏敏:	1
02:09:11	薛毓铨:	1
02:09:11	潘力:	1
02:10:41	邓 奇:	可以
02:10:45	徐楠:	1
02:12:04	孟明:	直方图均衡
02:12:06	Administrator:	老师，最后一步求和的时候，要排序吗，譬如根据与中心点6 的距离先排序再求和
02:12:09	Tang JM:	1
02:12:24	Tang JM:	comsum
02:12:35	蒋开文:	D(x)距离不是应该是两个点才能算出距离吗，这些点的距离是怎么算出来的？
02:12:53	唐陈:	以6为中心
02:12:55	蒋开文:	ok
02:12:58	杨俊桔:	以后每次算概率都要排除掉已选作中心的点吗
02:12:59	旭生 孙:	这最后一行算出来的意义是什么
02:13:13	王奕楷:	归一化
02:13:30	Tianhu Zhang:	 可以随机选点
02:13:54	陈悦:	2
02:13:56	马玄:	2
02:13:57	陈智杰:	 2
02:13:58	唐陈:	2
02:13:59	徐楠:	2
02:14:06	杨俊桔:	2
02:14:12	Administrator:	3
02:14:14	yangfanqihang:	2
02:14:15	qinzongding:	3
02:14:29	Yimin MA:	3-8
02:14:32	孙小婷:	3
02:14:39	薛毓铨:	好想说2.5
02:14:41	徐楠:	3
02:14:54	李文艺:	3
02:15:41	唐陈:	1
02:15:45	侠航 陈:	1
02:15:45	Song:	好玄哦
02:15:46	陈智杰:	再一边
02:15:51	Administrator:	老师，2和1的位置可以换的吗
02:16:02	陈智杰:	再一遍
02:16:02	严广宇:	sum的值，和计算点的顺序相关吧，这样换个顺序不就变了吗？
02:16:08	匡鸿深:	0.57是怎么定的
02:16:17	邓 奇:	这样选有啥意义吗？
02:16:29	齐鹏飞:	为什么随机选
02:17:16	齐鹏飞:	那要最后一行干嘛
02:17:23	齐鹏飞:	倒数第二行就行了
02:17:38	Administrator:	老师，你说的选中是指的啥
02:17:41	薛毓铨:	越远概率越大
02:17:58	杨洋:	如果把1234与5678的排序换一下会怎么样？
02:18:07	Baoce SUN:	为什么要sum？
02:18:14	严广宇:	概率密度函数？
02:18:16	匡鸿深:	与顺序无关
02:18:41	孟明:	选择远的有什么意义
02:18:45	Baoce SUN:	就是定个规则？
02:18:45	罗家伟:	那我把序号换一下，结果就不一样了
02:18:48	陈悦:	最后一行好像是轮盘赌求落点  程序好实现
02:18:49	李文艺:	工程上实现的
02:19:30	邓凯:	1
02:19:33	邓 奇:	1
02:19:35	Administrator:	老师，您刚刚这个0.57是随机的，那实际当中这个怎么定
02:19:38	陈靖韦:	那如果有三个点呢
02:19:39	Administrator:	？
02:19:43	齐鹏飞:	那要是随机数很小怎么办
02:19:45	李文艺:	离得远的概率区间大
02:19:49	齐鹏飞:	岂不是很近
02:20:10	杨俊桔:	即使换了顺序，小的求和也是小
02:20:33	杨俊桔:	OK
02:20:34	匡鸿深:	这是概率问题 与顺序无关
02:20:45	杨洋:	1
02:20:49	侠航 陈:	1
02:20:50	徐楠:	1
02:20:54	杨俊桔:	1
02:20:59	孟明:	选择之后呢
02:22:47	陈靖韦:	无监督的loss怎么算啊
02:23:03	严广宇:	距离呀
02:23:45	Administrator:	类内距离越小越好，类间距离越大越好
02:24:56	杨洋:	如果是三分类问题，选第2个点时计算和第1个点的距离，选第3个点时计算和第几个点的距离呢？
02:25:29	Song:	如果第二次选点，跟第一次重复了呢？
02:27:17	华涓 陈:	k近邻
02:27:33	唐陈:	特征近
02:29:21	华涓 陈:	kNN的k有什么好的确定方法么
02:29:37	王奕楷:	knn效率感人
02:29:43	齐鹏飞:	看数据
02:29:44	严广宇:	人为给的
02:30:07	匡鸿深:	K-means在实际应用中那些点是对应样本吗
02:30:10	高格格:	1
02:30:13	侠航 陈:	1
02:30:14	徐楠:	1
02:30:32	唐陈:	1
02:30:34	Administrator:	号
02:30:34	邓 奇:	好
02:31:01	WW BB:	10:25
02:37:01	唐陈:	1
02:37:01	孟明:	K-means++，直接选距离最远的那个点不会效果更好吗
02:37:01	陈韵莹:	1
02:37:01	邓 奇:	1
02:37:05	徐楠:	1
02:37:05	彭冰:	1
02:37:05	潘力:	1
02:37:06	陈悦:	1
02:37:06	侠航 陈:	1
02:37:07	高格格:	1
02:37:08	旭生 孙:	1
02:37:31	Song:	啥叫外点？
02:37:55	Song:	不好的点，就是单独成一类的点吗？
02:42:26	郑兴纯:	1
02:42:27	Tianhu Zhang:	1
02:42:27	卢宇宙:	1
02:42:27	彭冰:	1
02:42:27	杨俊桔:	1
02:42:28	Administrator:	啥叫超参数
02:42:29	韦俊:	1
02:42:30	严广宇:	1
02:42:32	徐楠:	1
02:42:34	Song:	那validation的比例是多少？
02:42:45	唐陈:	人为设定的参数
02:42:49	严广宇:	人为指定的参数就是超参数
02:42:53	Song:	哦哦。
02:42:58	王奕楷:	超参数一般像正则化这种
02:43:03	陈纬铨:	过
02:45:46	齐鹏飞:	测试的loss比训练的更小有这种情况吗，貌似遇过
02:45:48	陈智杰:	？
02:48:09	华涓 陈:	batch norm不会修改了数据原本应该的数据分布情况么
02:48:45	严广宇:	变换重构步骤会通过缩放因子恢复原始分布
02:49:03	齐鹏飞:	处理脏数据？
02:49:05	严广宇:	数据清洗？
02:49:41	罗家伟:	噪声？
02:50:03	齐鹏飞:	这个牛了
02:50:30	严广宇:	略微有一些噪声，可以提高泛化能力
02:52:12	华涓 陈:	有点惩罚参数的feel
02:53:37	匡鸿深:	这些噪声是真随机噪声还是有选择的筛选噪声
02:53:39	严广宇:	1
02:53:44	包颖:	所有的都瞎标，还是一部分瞎标？
02:54:08	齐鹏飞:	同一张图正负都有是不是就不行了
02:54:36	包颖:	那怎么知道哪个瞎标，哪个不是瞎标的呢。
02:54:37	李文艺:	可以用检测上吗？
02:54:50	邓凯:	论文链接可以分享一下吗
02:55:04	邓凯:	ok
02:55:05	齐鹏飞:	能提高多少呀
02:55:12	Song:	厉害呀！
02:55:20	李文艺:	检测也有label
02:56:51	陈靖韦:	1
02:56:52	唐陈:	1
02:56:55	杨俊桔:	1
02:58:08	彭冰:	1
02:58:10	陈靖韦:	1
02:59:05	杨俊桔:	1
02:59:10	Administrator:	方差
02:59:43	彭冰:	过拟合
02:59:45	Tianhu Zhang:	容易过拟
03:00:29	高格格:	对
03:00:30	Tianhu Zhang:	1
03:00:40	qinzongding:	看不懂
03:00:53	Song:	error是什么
03:00:58	陈靖韦:	loss
03:01:03	彭冰:	cost function
03:01:17	王睿:	1
03:01:19	邓凯:	1
03:01:19	李涛:	1
03:01:20	邓 奇:	1
03:01:23	qinzongding:	1
03:01:25	Baoce SUN:	1
03:01:29	Administrator:	是刚刚那个过拟合里面的variance？
03:01:35	夏敏:	老师，实际中，复杂度是怎么算出来的？然后lambda这个参数会这样从小到大去调试的吗？
03:02:08	邓凯:	1
03:02:13	杨俊桔:	1
03:02:13	Tianhu Zhang:	1
03:02:18	夏敏:	1
03:03:02	彭冰:	1
03:03:03	李涛:	1
03:03:05	杨俊桔:	1
03:03:15	Eric:	lamda跟复杂度没有关系吧。
03:04:39	杨洋:	1
03:04:39	邓凯:	没有
03:04:42	侠航 陈:	1
03:07:16	高格格:	1
03:07:18	邓凯:	1
03:07:22	Song:	方差是算什么变量的方差呀？
03:07:27	李林洲:	刚才没听清,s是什么
03:07:29	杨洋:	s是什么
03:07:42	杨洋:	sigma
03:07:44	李林洲:	ok
03:08:45	陈靖韦:	lr
03:09:55	杨洋:	1
03:09:56	王睿:	1
03:12:28	杨洋:	老师随机森林重要吗?
03:12:52	Song:	Decision Tree是解决什么问题的？
03:13:24	Song:	CNN可以完全打败 Decision Tree吗？
03:13:37	Song:	哈哈哈
03:13:38	彭冰:	哈哈
03:13:42	王睿:	hhh
03:13:49	马玄:	坐等CNN
03:14:05	王奕楷:	cv领域是这样的
03:14:07	彭冰:	说白了就是 CNN他们不会是吗
03:14:56	Song:	老师怎么什么都知道（实力吹捧）
03:15:31	邓凯:	要上主菜了
03:15:33	孟明:	如果不是为了面试的话，这些就不重要了？
03:15:42	薛毓铨:	那学ml纯粹是为了面试嘛 还是实际应用也有很多地方用到？
03:16:17	D:	老师，是下课了嘛。
03:16:22	Song:	老师，怎么感觉你什么都知道？
03:16:37	杨洋:	老师辛苦了，拜拜
03:16:40	邱松涛:	那XGBoost和XGBT这些呢
03:16:44	Song:	厉害了。
03:16:46	Brian:	老师能否讲讲刷题心得，感觉了
03:16:47	彭冰:	老师 完犊子了 我在30~40的区间 感觉凉了
03:16:49	侠航 陈:	老师辛苦
03:16:51	旭生 孙:	那pytorch和caffe什么时候能用的上
03:17:01	Song:	同问。
03:17:05	潘力:	好，老师辛苦了，谢谢老师
03:17:08	王奕楷:	xgb很有用，但不常用到cv
03:17:23	Song:	6啊！
03:17:30	齐鹏飞:	pytorch不适合加速呀发现
03:17:37	李文艺:	服务器什么时候用呢
03:17:42	杨洋:	身世浮沉雨打萍，老师你真有才，随口就来
03:17:44	薛毓铨:	这几周都是铺垫咯
03:17:54	王睿:	老师，cv里面细分领域的项目什么时候开始接触
03:17:59	王睿:	多少周
03:18:19	Song:	Pytorch“不适合加速”什么意思呀？
03:18:28	李文艺:	caffe要做什么项目呢？
03:18:48	夏敏:	老师，有没有可以将Pytorch的模型转换成Tensorflow或caffe的好用的方法
03:19:01	Yimin MA:	CNN的样本一般得是多大规模？
03:19:07	Brian:	cv里面的知识需要掌握的公式推导多吗？
03:19:14	Song:	今年团队要做横向，可以用Pytorch吗？
03:19:28	李文艺:	会提供数据集吗
03:19:35	王睿:	那对于细分领域选择会不会有一些建议介绍，感觉外行自己来选有点抓瞎
03:20:01	匡鸿深:	应用方向吧
03:20:13	Song:	那刚刚说“Pytorch不适合加速”会对项目有影响吗？
03:20:25	Song:	哦哦，好的。
03:20:28	齐鹏飞:	tensorRT很难呀
03:20:35	齐鹏飞:	很多自定义层
03:20:39	齐鹏飞:	pytorch的
03:20:45	齐鹏飞:	很难用RT加速
03:20:49	邱松涛:	pytorch和caffe学一个就行，还是两个都要
03:20:53	齐鹏飞:	caffe和tf更好
03:21:04	Song:	您刚刚说“层要一周”，“层”是什么意思呀？
03:21:20	薛毓铨:	辛苦老师 各位晚安 
03:21:24	李文艺:	训练技巧能总结一下文档吗
03:21:26	Song:	哦哦，好的。
03:22:11	王睿:	因为cv很广，选择cv应用细分方向的时候，比如找工作里各个应用方向的行业现状，就业需求，发展前景等
03:22:14	齐鹏飞:	辛苦啦
03:22:28	Song:	做横向，是买服务器划算，还是租云服务器划算呀？
03:22:36	王睿:	好的，谢谢老师
03:23:17	蒋开文:	老师邮箱是否可以提供一下
03:23:27	Song:	大公司一般是自己搭服务器还是买服务器呀？
03:23:37	Song:	喔喔。
03:23:56	Yimin MA:	弱弱问一下：翘了前面几周的作业，对CNN后面影响大吗？
03:24:15	严广宇:	哈哈哈
03:24:16	qinzongding:	老师，有带用GPU的服务器给我们做训练吗
03:24:36	Yimin MA:	嘻嘻，老师催作业太认真，我已经被批评了。
03:24:44	齐鹏飞:	我觉得校招会对细节问的比较细，社招可能对项目中真正用到的东西问的比较多
03:25:05	齐鹏飞:	哦哦
03:25:31	齐鹏飞:	嗯嗯
03:26:01	邱松涛:	社招没项目怎么办啊
03:26:03	李涛:	之后的作业可以用c++做吗
03:26:08	Brian:	刷题刷到怀疑智商。。怎么办。。
03:26:31	李涛:	刷leetcode也可以把
03:26:43	李文艺:	下周有啥项目呢？
03:26:48	Song:	现在听说AutoML很火，是这样吗？
03:27:01	高格格:	老师，非计算机专业的，想进入CV领域是不是好难啊？
03:27:10	齐鹏飞:	很难
03:27:15	邱松涛:	现在做作业什么的都是Python。工作里面用C++会比较多吗，C++的OpenCV要学一下吗
03:27:36	李涛:	c++有类似numpy这种库吗
03:28:14	李涛:	就是矩阵计算这种
03:28:20	李涛:	好的
03:28:52	Brian:	晚安
03:28:53	高格格:	老师再见
03:28:54	齐鹏飞:	嗯嗯
03:28:55	齐鹏飞:	晚安
03:28:59	Song:	sklearn是干嘛用的？
03:28:59	Tianhu Zhang:	老师再见
03:29:15	夏敏:	老师晚安
03:29:15	邓 奇:	辛苦老师，晚安
03:29:27	罗家伟:	同非计算机专业，感到担忧。。
03:30:10	罗家伟:	行，老师晚安
03:30:38	Song:	老师您是自学的吗？
03:30:54	Song:	哦哦，谢谢。
03:31:46	Song:	晚安。
